{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 12:45:34.377712: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-25 12:45:34.377742: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from sklearn import metrics\n",
    "\n",
    "from src.algorithms.Kmeans import Kmeans\n",
    "from src.algorithms.DBscan import DBscan\n",
    "from src.algorithms.isolation_forest import IsolationForest\n",
    "from src.algorithms.gan import GAN\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "import csv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_predicted, y_true):\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_predicted, y_true)\n",
    "    precision = sklearn.metrics.precision_score(y_predicted, y_true)\n",
    "    recall = sklearn.metrics.recall_score(y_predicted, y_true)\n",
    "    f1 = sklearn.metrics.f1_score(y_predicted, y_true)\n",
    "\n",
    "    return [confusion_matrix, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam_grid_Kmeans = {\\n    \"n_clusters\": np.arange(2, 4),\\n    \"treshold\": np.arange(0.1, 1, 0.1),\\n}\\n\\nparam_grid_DBscan = {\\n    \"eps\": np.arange(0.5, 1.5, 0.5),\\n    \"treshold\": np.arange(0.5, 2, 0.5),\\n    \"min_samples\": np.arange(1, 3, 1),\\n}\\n\\nalgorithms_params = [param_grid_Kmeans] \\nalgorithms = [\"Kmeans\"]\\n\\nmetrics = np.empty([2, 2, 5], dtype=object)\\n\\nfor i in range(1,2):\\n\\n    df_test = pd.read_csv(f\"data/sensor-cleaning-data/cleaned/test/reformatted/data1.csv\")\\n\\n    y_true = df_test[[\"error\"]]\\n\\n    for alg_id, alg in enumerate(algorithms_params):\\n        score = []\\n        for params in itertools.product(*alg.values()):\\n\\n            inner_dict = {k: v for (k, v) in zip(algorithms_params[alg_id].keys(), params)}\\n\\n            conf = {\\n                \"filtering\": \"None\",\\n                \"train_data\": f\"data/sensor-cleaning-data/cleaned/train/data{i}.csv\",\\n                \"input_vector_size\": 1,\\n                \"warning_stages\": [0.7, 0.9],\\n                **inner_dict,\\n                \"output\": [],\\n                \"output_conf\": [{}\\n                ],\\n            }\\n\\n            class_name = algorithms[alg_id]\\n\\n            class_ = globals()[class_name]\\n\\n            detector = class_(conf)\\n\\n            mask = []\\n            y_predicted = []\\n\\n            for idx, row in df_test.iterrows():\\n\\n                status_code = detector.message_insert(\\n                    {\\n                        \"timestamp\": df_test[\"offset\"].iloc[idx],\\n                        \"ftr_vector\": [df_test[\"val\"].iloc[idx]],\\n                    }\\n                )\\n\\n                if status_code == 2:\\n                    y_predicted.append(False)\\n                if status_code == 1:\\n                    y_predicted.append(False)\\n                elif status_code == -1:\\n                    y_predicted.append(True)\\n\\n\\n            m = compute_metrics(y_predicted, y_true)\\n            m.append(params)\\n            score.append(m)\\n            print(m)\\n            #print(f\"{algorithms[alg_id]}\", score)\\n        #print(max(score, key=lambda x: x[0]))\\n        # max_list = max(score, key=lambda x: x[3])\\n\\n        # metrics[alg_id, i-1, :] = max_list\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "param_grid_Kmeans = {\n",
    "    \"n_clusters\": np.arange(2, 4),\n",
    "    \"treshold\": np.arange(0.1, 1, 0.1),\n",
    "}\n",
    "\n",
    "param_grid_DBscan = {\n",
    "    \"eps\": np.arange(0.5, 1.5, 0.5),\n",
    "    \"treshold\": np.arange(0.5, 2, 0.5),\n",
    "    \"min_samples\": np.arange(1, 3, 1),\n",
    "}\n",
    "\n",
    "algorithms_params = [param_grid_Kmeans] \n",
    "algorithms = [\"Kmeans\"]\n",
    "\n",
    "metrics = np.empty([2, 2, 5], dtype=object)\n",
    "\n",
    "for i in range(1,2):\n",
    "\n",
    "    df_test = pd.read_csv(f\"data/sensor-cleaning-data/cleaned/test/reformatted/data1.csv\")\n",
    "\n",
    "    y_true = df_test[[\"error\"]]\n",
    "\n",
    "    for alg_id, alg in enumerate(algorithms_params):\n",
    "        score = []\n",
    "        for params in itertools.product(*alg.values()):\n",
    "\n",
    "            inner_dict = {k: v for (k, v) in zip(algorithms_params[alg_id].keys(), params)}\n",
    "\n",
    "            conf = {\n",
    "                \"filtering\": \"None\",\n",
    "                \"train_data\": f\"data/sensor-cleaning-data/cleaned/train/data{i}.csv\",\n",
    "                \"input_vector_size\": 1,\n",
    "                \"warning_stages\": [0.7, 0.9],\n",
    "                **inner_dict,\n",
    "                \"output\": [],\n",
    "                \"output_conf\": [{}\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            class_name = algorithms[alg_id]\n",
    "\n",
    "            class_ = globals()[class_name]\n",
    "\n",
    "            detector = class_(conf)\n",
    "\n",
    "            mask = []\n",
    "            y_predicted = []\n",
    "\n",
    "            for idx, row in df_test.iterrows():\n",
    "\n",
    "                status_code = detector.message_insert(\n",
    "                    {\n",
    "                        \"timestamp\": df_test[\"offset\"].iloc[idx],\n",
    "                        \"ftr_vector\": [df_test[\"val\"].iloc[idx]],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if status_code == 2:\n",
    "                    y_predicted.append(False)\n",
    "                if status_code == 1:\n",
    "                    y_predicted.append(False)\n",
    "                elif status_code == -1:\n",
    "                    y_predicted.append(True)\n",
    "\n",
    "\n",
    "            m = compute_metrics(y_predicted, y_true)\n",
    "            m.append(params)\n",
    "            score.append(m)\n",
    "            print(m)\n",
    "            #print(f\"{algorithms[alg_id]}\", score)\n",
    "        #print(max(score, key=lambda x: x[0]))\n",
    "        # max_list = max(score, key=lambda x: x[3])\n",
    "\n",
    "        # metrics[alg_id, i-1, :] = max_list\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_test = pd.read_csv(f\"data/sensor-cleaning-data/cleaned/test/reformatted/data1.csv\")\\n# select to columns from dataframe\\nX = df_test[[\"offset\", \"val\"]]\\ny = df_test[[\"error\"]]\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_test = pd.read_csv(f\"data/sensor-cleaning-data/cleaned/test/reformatted/data1.csv\")\n",
    "# select to columns from dataframe\n",
    "X = df_test[[\"offset\", \"val\"]]\n",
    "y = df_test[[\"error\"]]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        train_data=\"data/cleaned/train/data1.csv\",\n",
    "        test_data = \"data/cleaned/test/reformatted/data1.csv\",\n",
    "        alg = \"Kmeans\",\n",
    "\n",
    "        # Kmeans \n",
    "        n_clusters=1,\n",
    "        treshold=1,\n",
    "\n",
    "        #DBscan\n",
    "        eps = 0.5,\n",
    "        db_treshold=1,\n",
    "        min_samples = 2,\n",
    "\n",
    "        #IsolationForest\n",
    "        max_samples = 100,\n",
    "        max_features=1,\n",
    "        contamination = 0.05,\n",
    "        # GAN\n",
    "        N_latent=3,\n",
    "        K=8,\n",
    "        len_window=500\n",
    "        \n",
    "       \n",
    "    ):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.alg = alg\n",
    "\n",
    "        # Kmeans\n",
    "        self.n_clusters = n_clusters\n",
    "        self.treshold = treshold\n",
    "\n",
    "        # DBscan\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.db_treshold = db_treshold\n",
    "\n",
    "        # IsolationForest\n",
    "        self.max_samples = max_samples \n",
    "        self.max_features = max_features\n",
    "        self.contamination = contamination\n",
    "\n",
    "        # GAN\n",
    "        self.N_latent = N_latent\n",
    "        self.K = K\n",
    "        self.len_window = len_window\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        inner_dict = {\n",
    "            \"train_data\": self.train_data,\n",
    "            # Kmeans\n",
    "            \"n_clusters\": self.n_clusters,\n",
    "            \"treshold\": self.treshold,\n",
    "            # DBscan\n",
    "            \"eps\": self.eps,\n",
    "            \"min_samples\": self.min_samples,\n",
    "            \"db_treshold\": self.db_treshold,\n",
    "            # IsolationForest\n",
    "            \"max_samples\": self.max_samples,\n",
    "            \"max_features\": self.max_features,\n",
    "            \"contamination\": self.contamination,\n",
    "            # GAN\n",
    "            \"N_latent\": self.N_latent,\n",
    "            \"K\": self.K,\n",
    "            \"len_window\": self.len_window,\n",
    "        }\n",
    "\n",
    "        conf = {\n",
    "            \"filtering\": \"None\",\n",
    "            \"input_vector_size\": 1,\n",
    "            \"warning_stages\": [0.7, 0.9],\n",
    "            \"model_name\":\"IsolationForest\",\n",
    "            **inner_dict,\n",
    "            \"output\": [],\n",
    "            \"output_conf\": [{}],\n",
    "        }\n",
    "\n",
    "        print(conf)\n",
    "\n",
    "        class_ = globals()[self.alg]\n",
    "        self.detector_ = class_(conf)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        print(X.shape, self.alg, self.contamination, self.max_samples, self.max_features, self.train_data, self.test_data)\n",
    " \n",
    "\n",
    "\n",
    "        y_predicted = []\n",
    "\n",
    "        df_test = pd.read_csv(self.test_data)\n",
    "\n",
    "        # transverse X rows\n",
    "        for idx, row in X.iterrows():\n",
    "            message = {\n",
    "                \"timestamp\": df_test[\"offset\"].iloc[idx],\n",
    "                \"ftr_vector\": [df_test[\"val\"].iloc[idx]],\n",
    "            }\n",
    "\n",
    "            status_code = self.detector_.message_insert(message)\n",
    "\n",
    "            if status_code == 2:\n",
    "                y_predicted.append(False)\n",
    "            if status_code == 1:\n",
    "                y_predicted.append(False)\n",
    "            elif status_code == -1:\n",
    "                y_predicted.append(True)\n",
    "\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sensor-cleaning-data/cleaned/test/reformatted/data7.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alg \u001b[38;5;129;01min\u001b[39;00m algorithms:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m         df_test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/sensor-cleaning-data/cleaned/test/reformatted/data\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# select to columns from dataframe\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         X \u001b[38;5;241m=\u001b[39m df_test[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    608\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:462\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    459\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:819\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:1050\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:1867\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1864\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m-> 1867\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/parsers.py:1362\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/a_det2/lib/python3.8/site-packages/pandas/io/common.py:642\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m         errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sensor-cleaning-data/cleaned/test/reformatted/data7.csv'"
     ]
    }
   ],
   "source": [
    "algorithms = [\"DBscan\"]\n",
    "\n",
    "DBscan_params = {\n",
    "    \n",
    "    \"alg\": [\"DBscan\"],\n",
    "    # Kmeans\n",
    "    \"n_clusters\": [2],\n",
    "    \"treshold\": [0.5],\n",
    "    # DBscan\n",
    "    \"eps\": np.arange(0.1, 1, 0.1),\n",
    "    \"db_treshold\": np.arange(0.05, 1, 0.05),\n",
    "    \"min_samples\": np.arange(15, 50, 15),\n",
    "    # IsolationForest\n",
    "    \"max_samples\": [100],\n",
    "    # GAN\n",
    "    \"N_latent\": [3],\n",
    "    \"K\": [8],\n",
    "    \"len_window\": [500],\n",
    "}\n",
    "\n",
    "Kmeans_params = {\n",
    "    \n",
    "    \"alg\": [\"Kmeans\"],\n",
    "    # Kmeans\n",
    "    \"n_clusters\": [2, 3, 4],\n",
    "    \"treshold\": np.arange(0.05, 1, 0.05),\n",
    "    # DBscan\n",
    "    \"eps\": [0.5],\n",
    "    \"db_treshold\": [0.5],\n",
    "    \"min_samples\": [100],\n",
    "    # IsolationForest\n",
    "    \"max_samples\": [100],\n",
    "    # GAN\n",
    "    \"N_latent\": [3],\n",
    "    \"K\": [8],\n",
    "    \"len_window\": [500],\n",
    "}\n",
    "\n",
    "IsolationForest_params = {\n",
    "    \"alg\": [\"IsolationForest\"],\n",
    "    # Kmeans\n",
    "    \"n_clusters\": [2],\n",
    "    \"treshold\": [0.5],\n",
    "    # DBscan\n",
    "    \"eps\": [0.5],\n",
    "    \"db_treshold\": [0.5],\n",
    "    \"min_samples\": [100],\n",
    "    # IsolationForest\n",
    "    \"max_samples\": np.arange(2500, 10001, 2500),\n",
    "    \"max_features\": [1],\n",
    "    \"contamination\": np.arange(0.001, 0.01, 0.001),\n",
    "    # GAN\n",
    "    \"N_latent\": [3],\n",
    "    \"K\": [8],\n",
    "    \"len_window\": [500],\n",
    "}\n",
    "\n",
    "\n",
    "for alg in algorithms:\n",
    "\n",
    "    for i in range(7, 10):\n",
    "\n",
    "        df_test = pd.read_csv(f\"data/cleaned/test/reformatted/data{i}.csv\")\n",
    "        # select to columns from dataframe\n",
    "        X = df_test[[\"offset\", \"val\"]]\n",
    "        y = df_test[[\"error\"]]\n",
    "\n",
    "        print(\"Length of X and y\", X.shape, y.shape)\n",
    "\n",
    "        test_params = {\n",
    "            **eval(f\"{alg}_params\"),\n",
    "            \"train_data\": [f\"data/cleaned/train/data{i}.csv\"],\n",
    "            \"test_data\": [\n",
    "                f\"data/cleaned/test/reformatted/data{i}.csv\"\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        estimator = Estimator()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "            estimator,\n",
    "            param_grid=test_params,\n",
    "            scoring=\"f1\",\n",
    "            \n",
    "            cv=TimeSeriesSplit(n_splits=2),\n",
    "        )\n",
    "\n",
    "        print(\"Length of X and y\", X.shape, y.shape)\n",
    "        print(X.iloc[:10])\n",
    "\n",
    "        selected = clf.fit(X, y)\n",
    "\n",
    "        print(\"Length \", X.shape, y.shape)\n",
    "        print(selected)\n",
    "\n",
    "        best_estimator = clf.best_estimator_\n",
    "        print(best_estimator)\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        print(y_pred)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, y_pred)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        comp_metrics = compute_metrics(y_pred, y)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc}\\n Conf_matrix={comp_metrics[0]}\\n Precision={comp_metrics[1]}\\n Recall={comp_metrics[2]}\\n F1={comp_metrics[3]}\",)\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend()\n",
    "        alg_name = selected.cv_results_[\"params\"][0][\"alg\"]\n",
    "        plt.savefig(f'results_1/{alg_name}-data{i}.png')\n",
    "        plt.close\n",
    "\n",
    "        transposed_data = zip(*[selected.cv_results_[key] for key in selected.cv_results_])\n",
    "        is_empty = (\n",
    "            not os.path.exists(f\"results_1/{alg_name}.csv\")\n",
    "            or os.path.getsize(f\"results_1/{alg_name}.csv\") == 0\n",
    "        )\n",
    "\n",
    "        with open(f\"results_1/{alg_name}.csv\", \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "\n",
    "            # Write headers only if the file is empty\n",
    "            if is_empty:\n",
    "                writer.writerow(selected.cv_results_.keys())\n",
    "\n",
    "            # Write data rows\n",
    "            writer.writerows(transposed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimator(max_features=1, contamination=0.01, max_samples=100).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Estimator().fit(X,y).predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.cv_results_[\"params\"][0][\"alg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = list(selected.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in selected.cv_results_:\n",
    "    print(key)\n",
    "    print(type(selected.cv_results_[key]))\n",
    "    print(len(selected.cv_results_[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = clf.best_estimator_\n",
    "y_pred = best_estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(y_pred, y)\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc}\\n Conf_matrix={metrics[0]}\\n Precision={metrics[1]}\\n Recall={metrics[2]}\\n F1={metrics[3]}\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_data = zip(*[selected.cv_results_[key] for key in selected.cv_results_])\n",
    "\n",
    "\n",
    "with open(\"results_1/Kmeans.csv\", \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Write headers\n",
    "    writer.writerow(selected.cv_results_.keys())\n",
    "\n",
    "    # Write data rows\n",
    "    writer.writerows(transposed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_1/IsolationForest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the DataFrame\n",
    "print(len(df))\n",
    "\n",
    "# Check the index of the row with the maximum mean test score\n",
    "print(df[\"mean_test_score\"].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df[df[\"param_train_data\"] == \"data/sensor-cleaning-data/cleaned/train/data1.csv\"][\"mean_test_score\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df[df[\"param_train_data\"] == \"data/sensor-cleaning-data/cleaned/train/data1.csv\"][\"split1_test_score\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(15)['param_train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_det2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
